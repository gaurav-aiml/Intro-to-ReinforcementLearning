{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this project, the environment of the reinforcement learning project is built in a Grid World. There are two environments based on Grid World. A Deterministic environment and a Stochastic Environment. It as been assumed that the structure of the environment is known and hence Value Iteration is used to solve the control problem of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Components in Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reinforcement Learing mainly contains two entities\n",
    "1. Agent\n",
    "2. Environment\n",
    "3. Reward\n",
    "4. State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " <img src=\"./files/components.png\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**1. Agent**: The agent is the entity that learns and takes actions inside the environment. The main goal of the agent is to maximize the reward it obtains of a period of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**2. Environment**: The Environment is the entity that gives rewards to the agents and also gives the state in which the Agent ends up in after the Agent interacts with the environment by taking action. There are Reinforcement Problems wherein the strucutre and behaviour of the Environment are known (Model_Based) and some Evnironments where the behaviour of the environment is unknown (Model-Free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**3. Reward**: The reward is a single integer value returned to the agent by the environment as result of the agents interaction (actions) with the environment. The agents aims to increase these rewards in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**4. State**: The Environment at each instant of time is called a state. The agents moves from one state to another by performing actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Types of Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this project. Two kinds of environments are solved. \n",
    "1. Deterministic: In this kind of environment, given the current state of the environment and the action of the agent the next state to which the agent transitions is fixed. Also every time the agents takes an action from a particular state, the immediate reward recieved will be same all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. Stochastic Environment: In this kind of an environment, given current state and the action of the agent taken fron that state, the state in which the agent ends up is not certain. There may be more than one possible states the agents might end up after taking a particular action. Also, in a stochastic environment every time the agents takes a particular action from a particular state, the immediate reward received might not be the same since there are more than one possible state the agent ends up in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Markov Decision Process (MPD) is a Markov Reward Process with decisions (because actions are involved). In this each process is a Markov Process. MDP tuple is **(S, A, P, R, ùõæ)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. **S**: State Space\n",
    "2. **A**: A set of finite actions that can be taken by the agent\n",
    "3. **P**: State Transition Probabilities Function\n",
    "4. **R**: Reward Function\n",
    "5. **ùõæ**: Discount Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The follwing formula denote P and R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"./files/trans_prob_function.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " <img src=\"./files/reward_function.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this project, two kinds of environments are used to illustrate the value iteration to solve the control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Deterministic Grid World**: The agent (car) in this environment moves through a 3x4 grid. The final terminal (winning) state is S(0,3) which gives a reward of +10 and there is a state just below the final state, which is bad state to be in since there is a raging fire in the state and going there would give a reward of -5. There is also another terminal state (1,1) which is a game over state (if the agent goes into that state, it is stuck)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Stochastic Grid World**: This environment is a slightly modified from the Deterministic Grid World, wherein we assume the car faulty steering wheele and hence, when aciton \"right\" or \"left\" is taken, there is 80% chance that bheaves accordinly, and there is chance of 15% that the car goes to a state that is above and 5% chance the car goes to a state that is below, similarly when action \"up\" and \"down\" is taken the car goes right with a 17% chance and goes left with 3% chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dynamic Programming (Value Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this project, Dynamic Programming (Value Iteration) is used to find the optimal policy. This method assumes full knowledge of the MDP (knowledge of how to the environment works) it is a model-based method of solving the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This planning method is based on the Optimality Theorm \n",
    "\n",
    "\n",
    "**A policy ùúã(a|s) achieves the optimal value from state s, i.e. Vùúã(s) = V\\*(s), if and only if, for any state s‚Ä≤ reachable from s, ùúã achieves the optimal value from state s‚Ä≤, Vùúã(s‚Ä≤) = V\\*(s‚Ä≤).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the solution to V*(s') is known, then then V*(s) can be found by one step look ahead as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " <img src=\"./files/bellman_optimality_equation.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Further, this bellman optimality eqation can be iteratively applied until the values converge to give an optimal value for each state of the environment. After this, the agent can act greedily (to select the action that reaches the state with the highest value at each time step to reach the winning state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"./files/bellman_iterative.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Please note: There are utility functions and other dependencies that work only when cells are run in a sequential order. While evaluating to reproduce the results kindly run the cells seqeuntially**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deterministic Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Utility Functions for visualization of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\\n---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.row = start[0]\n",
    "        self.col = start[1]\n",
    "        \n",
    "    def step(self, action):\n",
    "        if action in self.actions[(self.row, self.col)]:\n",
    "            if action == \"L\":\n",
    "                self.row -= 1\n",
    "            elif action == \"R\":\n",
    "                self.row += 1\n",
    "            elif action == \"U\":\n",
    "                self.col += 1\n",
    "            elif action == \"D\":\n",
    "                self.col -= 1\n",
    "            return self.rewards.get((self.row, self.col), 0)\n",
    "            \n",
    "            \n",
    "    def reset(self, rewards, actions):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def find_next_state_reward(self, action):\n",
    "        agent_pos_row = self.row\n",
    "        agent_pos_col = self.col\n",
    "        if action in self.actions[(agent_pos_row, agent_pos_col)]:\n",
    "            if action == \"U\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row-1, agent_pos_col):\n",
    "                    agent_pos_row -= 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"D\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row+1, agent_pos_col ):\n",
    "                    agent_pos_row += 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"R\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row, agent_pos_col+1):\n",
    "                    agent_pos_col += 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"L\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row, agent_pos_col-1):\n",
    "                    agent_pos_col -= 1 \n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        reward = self.rewards.get((agent_pos_row, agent_pos_col), 0)\n",
    "        return ((agent_pos_row, agent_pos_col), reward)\n",
    "    \n",
    "    def check_boundary_and_invalid(self,row, col):\n",
    "        if row >=0 and row <= self.height:\n",
    "                if col >=0 and col <= self.width:\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    def non_terminal_states(self):\n",
    "        return self.actions.keys()\n",
    "\n",
    "    def set_agent_position(self, s):\n",
    "        self.row = s[0]\n",
    "        self.col = s[1]\n",
    "        \n",
    "    \n",
    "    def trans_probs(self, action):\n",
    "        # returns a transition probablity, reward, state tuple\n",
    "        trans_probs = []\n",
    "        state, reward = self.find_next_state_reward(action)\n",
    "        trans_probs.append((1, reward, state))\n",
    "        return trans_probs\n",
    "    \n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above class is used to build the Deterministic Environment Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, grid, discount):\n",
    "        self.environment = grid\n",
    "        self.state_value = {}\n",
    "        self.action_value = {}\n",
    "        self.valid_actions = (\"U\",\"D\",\"L\",\"R\") # (Up, Down, Left, Right)\n",
    "        self.discount = discount\n",
    "        \n",
    "        for s in grid.all_states():\n",
    "            self.state_value[s] = 0\n",
    "        \n",
    "    \n",
    "    def find_value_action(self, s):\n",
    "        #Function to find the optimal state value of the current state based on the state value of the next state from previous iteration\n",
    "        best_action = \"GO\"\n",
    "        best_value = float('-inf')\n",
    "        self.environment.set_agent_position(s)\n",
    "        legal_actions = self.environment.actions[s]\n",
    "        for action in legal_actions:\n",
    "            transitions = self.environment.trans_probs(action)\n",
    "            expected_value = 0\n",
    "            expected_reward = 0\n",
    "            for (prob, immediate_reward, next_state) in transitions:\n",
    "                expected_reward += prob * immediate_reward\n",
    "                expected_value += prob * self.state_value[next_state]\n",
    "            current_value = expected_reward + discount * expected_value\n",
    "            if current_value > best_value:\n",
    "                best_value = current_value\n",
    "                best_action = action\n",
    "        return best_action, best_value\n",
    "\n",
    "    def find_v(self):\n",
    "        states = self.environment.all_states()\n",
    "        for s in self.environment.non_terminal_states():\n",
    "            previous_state_value = self.state_value[s]\n",
    "            _, new_state_value = self.find_value_action(s)\n",
    "            self.state_value[s] = new_state_value\n",
    "        return self.state_value\n",
    "\n",
    "    def initialize_random_policy(self):\n",
    "        policy = {}\n",
    "        for s in self.environment.non_terminal_states():\n",
    "            policy[s] = np.random.choice(self.valid_actions)\n",
    "        return policy\n",
    "\n",
    "    def calculate_greedy_policy(self,V):\n",
    "        policy = self.initialize_random_policy()\n",
    "        for s in policy.keys():\n",
    "            self.environment.set_agent_position(s)\n",
    "        # consider actions to find the best current action\n",
    "            best_action, _ = self.find_value_action(s)\n",
    "            policy[s] = best_action\n",
    "        return policy\n",
    "    \n",
    "    def value_iteration(self, iterations):\n",
    "        print(\"Rewards:\")\n",
    "        print_values(self.environment.rewards, self.environment)\n",
    "        for i in range(iterations):\n",
    "            print(\"\\n------- Iteration {}-----------\\n\".format(i))\n",
    "            values = self.find_v()\n",
    "            print(\"Values:\")\n",
    "            print_values(values, self.environment)\n",
    "            \n",
    "        greedy_policy = self.calculate_greedy_policy(values)\n",
    "        print(\"\\npolicy:\")\n",
    "        print_policy(greedy_policy, self.environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since it is assumed the full knowledge of MDP for value iteration, the actions are given expilictly while creating the grid object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def grid():\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 10, \n",
    "               (1, 3): -5,}\n",
    "    \n",
    "    actions = { (0, 0): ('D', 'R'), (0, 1): ('L', 'R'), (0, 2): ('L', 'D', 'R'), (1, 0): ('U', 'D'), (1,1): (),\n",
    "    (1, 2): ('U', 'D', 'R'), (1,3) : (\"L\", \"D\"),  (2, 0): ('U', 'R'), (2, 1): ('L', 'R'),(2, 2): ('L', 'R', 'U'),(2, 3): ('L', 'U'),}\n",
    "    g.reset(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = grid()\n",
    "a = Agent(g, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Results of Deterministic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 10.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-5.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "------- Iteration 0-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 0.00| 0.00| 10.00| 0.00|\n",
      "---------------------------\n",
      " 0.00|-inf| 5.00| 2.50|\n",
      "---------------------------\n",
      " 0.00| 0.00| 2.50| 1.25|\n",
      "\n",
      "------- Iteration 1-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 0.00| 5.00| 10.00| 0.00|\n",
      "---------------------------\n",
      " 0.00|-inf| 5.00| 2.50|\n",
      "---------------------------\n",
      " 0.00| 1.25| 2.50| 1.25|\n",
      "\n",
      "------- Iteration 2-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 2.50| 5.00| 10.00| 0.00|\n",
      "---------------------------\n",
      " 1.25|-inf| 5.00| 2.50|\n",
      "---------------------------\n",
      " 0.62| 1.25| 2.50| 1.25|\n",
      "\n",
      "------- Iteration 3-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 2.50| 5.00| 10.00| 0.00|\n",
      "---------------------------\n",
      " 1.25|-inf| 5.00| 2.50|\n",
      "---------------------------\n",
      " 0.62| 1.25| 2.50| 1.25|\n",
      "\n",
      "policy:\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |  GO  |  U  |  L  |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "a.value_iteration(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above cell shows the value iteration for 4 iterations and in the end when the agent acts greedily to give out an optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stochastic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.row = start[0]\n",
    "        self.col = start[1]\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        #A Slight variation of the deterministic environment where each action is stochastic in nature\n",
    "        if self.stoch_prob < 1:\n",
    "            p = np.random.random()\n",
    "            if p <= self.stoch_prob:\n",
    "                pass\n",
    "            else:\n",
    "                if action == \"L\" or action == \"R\":\n",
    "                    action = np.random.choice([\"U\", \"D\"], p =[0.15, 0.05])\n",
    "                    #probability of left or right = 0.8, probability of up = 0.15, down = 0.05\n",
    "                elif action == \"U\" or action == \"D\":\n",
    "                    action = np.random.choice([\"L\", \"R\"], p=[0.03, 0.17])\n",
    "                    #probability of up or down = 0.8, probability of right = 0.17, left = 0.03\n",
    "\n",
    "                \n",
    "        if action in self.actions[(self.row, self.col)]:\n",
    "            if action == \"L\":\n",
    "                self.row -= 1\n",
    "            elif action == \"R\":\n",
    "                self.row += 1\n",
    "            elif action == \"U\":\n",
    "                self.col += 1\n",
    "            elif action == \"D\":\n",
    "                self.col -= 1\n",
    "            return self.rewards.get((self.row, self.col), 0)\n",
    "            \n",
    "            \n",
    "    def reset(self, rewards, actions, stoch_prob):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.stoch_prob = stoch_prob\n",
    "        \n",
    "    def find_next_state_reward(self, action):\n",
    "        agent_pos_row = self.row\n",
    "        agent_pos_col = self.col\n",
    "        if action in self.actions[(agent_pos_row, agent_pos_col)]:\n",
    "            if action == \"U\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row-1, agent_pos_col):\n",
    "                    agent_pos_row -= 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"D\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row+1, agent_pos_col ):\n",
    "                    agent_pos_row += 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"R\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row, agent_pos_col+1):\n",
    "                    agent_pos_col += 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif action == \"L\":\n",
    "                if self.check_boundary_and_invalid(agent_pos_row, agent_pos_col-1):\n",
    "                    agent_pos_col -= 1 \n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        reward = self.rewards.get((agent_pos_row, agent_pos_col), 0)\n",
    "        return ((agent_pos_row, agent_pos_col), reward)\n",
    "    \n",
    "    def check_boundary_and_invalid(self,row, col):\n",
    "        if row >=0 and row <= self.height:\n",
    "                if col >=0 and col <= self.width:\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    def non_terminal_states(self):\n",
    "        return self.actions.keys()\n",
    "\n",
    "    def set_agent_position(self, s):\n",
    "        self.row = s[0]\n",
    "        self.col = s[1]\n",
    "    \n",
    "    def trans_probs(self, action):\n",
    "        # returns a transition probablity, reward, state tuple\n",
    "        trans_probs = []\n",
    "        state, reward = self.find_next_state_reward(action)\n",
    "        trans_probs.append((self.stoch_prob, reward, state))\n",
    "        stoch_prob_inv = 1 - self.stoch_prob\n",
    "\n",
    "        if (stoch_prob_inv == 0.0):\n",
    "            return trans_probs\n",
    "        \n",
    "        #Below lines incorporate stochasticity into the actions taken by the agent\n",
    "        if action == \"L\" or action == \"R\":\n",
    "            state, reward = self.find_next_state_reward(\"U\")\n",
    "            trans_probs.append((0.15, reward, state)) \n",
    "            state, reward = self.find_next_state_reward(\"D\")\n",
    "            trans_probs.append((0.05, reward, state))\n",
    "        elif action == \"U\" or action == \"D\":\n",
    "            state, reward = self.find_next_state_reward(\"L\")\n",
    "            trans_probs.append((0.03, reward, state))\n",
    "            state, reward = self.find_next_state_reward(\"R\")\n",
    "            trans_probs.append((0.17, reward, state))\n",
    "        return trans_probs\n",
    "    \n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, grid, discount):\n",
    "        self.environment = grid\n",
    "        self.state_value = {}\n",
    "        self.action_value = {}\n",
    "        self.valid_actions = (\"U\",\"D\",\"L\",\"R\") # (Up, Down, Left, Right)\n",
    "        self.discount = discount\n",
    "        \n",
    "        for s in grid.all_states():\n",
    "            self.state_value[s] = 0\n",
    "        \n",
    "    \n",
    "    def find_value_action(self, s):\n",
    "        best_action = \"GO\"\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        self.environment.set_agent_position(s)\n",
    "        legal_actions = self.environment.actions[s]\n",
    "        for action in legal_actions:\n",
    "            transitions = self.environment.trans_probs(action)\n",
    "            expected_value = 0\n",
    "            expected_reward = 0\n",
    "            for (prob, immediate_reward, next_state) in transitions:\n",
    "                expected_reward += prob * immediate_reward\n",
    "                expected_value += prob * self.state_value[next_state]\n",
    "            current_value = expected_reward + discount * expected_value\n",
    "            if current_value > best_value:\n",
    "                best_value = current_value\n",
    "                best_action = action\n",
    "        return best_action, best_value\n",
    "\n",
    "    def find_v(self):\n",
    "        states = self.environment.all_states()\n",
    "        for s in self.environment.non_terminal_states():\n",
    "            previous_state_value = self.state_value[s]\n",
    "#             print(\"previous_state:\",previous_state_value)\n",
    "            _, new_state_value = self.find_value_action(s)\n",
    "#             print(\"new_state:\", new_state_value)\n",
    "            self.state_value[s] = new_state_value\n",
    "        return self.state_value\n",
    "\n",
    "    def initialize_random_policy(self):\n",
    "        policy = {}\n",
    "        for s in self.environment.non_terminal_states():\n",
    "            policy[s] = np.random.choice(self.valid_actions)\n",
    "        return policy\n",
    "\n",
    "    def calculate_greedy_policy(self,V):\n",
    "        policy = self.initialize_random_policy()\n",
    "        for s in policy.keys():\n",
    "            self.environment.set_agent_position(s)\n",
    "            best_action, _ = self.find_value_action(s)\n",
    "            policy[s] = best_action\n",
    "        return policy\n",
    "    \n",
    "    def value_iteration(self, iterations):\n",
    "        print(\"Rewards:\")\n",
    "        print_values(self.environment.rewards, self.environment)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(\"\\n------- Iteration {}-----------\\n\".format(i))\n",
    "            values = self.find_v()\n",
    "            print(\"Values:\")\n",
    "            print_values(values, self.environment)\n",
    "            \n",
    "        greedy_policy = self.calculate_greedy_policy(values)\n",
    "        print(\"\\npolicy:\")\n",
    "        print_policy(greedy_policy, self.environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since it is assumed the full knowledge of MDP for value iteration, the actions are given expilictly while creating the grid object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def grid(stoch_prob):\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 10, \n",
    "               (1, 3): -5,}\n",
    "    actions = { (0, 0): ('D', 'R'), (0, 1): ('L', 'R'), (0, 2): ('L', 'D', 'R'), (1, 0): ('U', 'D'), (1,1): (),\n",
    "    (1, 2): ('U', 'D', 'R'), (1,3) : (\"L\", \"D\"),  (2, 0): ('U', 'R'), (2, 1): ('L', 'R'),(2, 2): ('L', 'R', 'U'),(2, 3): ('L', 'U'),}\n",
    "    g.reset(rewards, actions, stoch_prob)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the stochastic environment, a variable for the stochasticity is introduced which makes the actions of the agents stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = grid(0.8)\n",
    "a = Agent(g, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Results of Stochastic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 10.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-5.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "------- Iteration 0-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 0.00| 0.00| 8.00| 0.00|\n",
      "---------------------------\n",
      " 0.00|-inf| 2.35| 0.19|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.94|-0.36|\n",
      "\n",
      "------- Iteration 1-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 0.00| 3.20| 8.66| 0.00|\n",
      "---------------------------\n",
      " 0.00|-inf| 2.66| 0.32|\n",
      "---------------------------\n",
      " 0.00| 0.38| 1.04|-0.32|\n",
      "\n",
      "------- Iteration 2-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 1.28| 3.78| 8.72| 0.00|\n",
      "---------------------------\n",
      " 0.51|-inf| 2.70| 0.35|\n",
      "---------------------------\n",
      " 0.24| 0.45| 1.06|-0.31|\n",
      "\n",
      "------- Iteration 3-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 1.62| 3.86| 8.72| 0.00|\n",
      "---------------------------\n",
      " 0.70|-inf| 2.71| 0.35|\n",
      "---------------------------\n",
      " 0.32| 0.47| 1.06|-0.31|\n",
      "\n",
      "------- Iteration 4-----------\n",
      "\n",
      "Values:\n",
      "---------------------------\n",
      " 1.69| 3.87| 8.72| 0.00|\n",
      "---------------------------\n",
      " 0.74|-inf| 2.71| 0.35|\n",
      "---------------------------\n",
      " 0.34| 0.47| 1.06|-0.31|\n",
      "\n",
      "policy:\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |  GO  |  U  |  L  |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "a.value_iteration(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
